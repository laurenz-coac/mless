{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "29fe2d4d",
      "metadata": {
        "id": "29fe2d4d"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maschu09/mless/blob/main/time_series_forecasting/5_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92e6157c",
      "metadata": {},
      "source": [
        "# The Approaches:\n",
        "\n",
        "If we want to incorporate future temperature data to predict current ozone measurements, we have two approaches that do not require architectural changes:\n",
        "\n",
        "1. **Shifting the temperature measurements backwards:** Say our context window starts at $t=0$ and goes until timepoint $t=w_c$. Our prediciton window then, of course, starts at $t=w_c +1$ and goes until $t=w_c + w_p$, where $w_p$ is the length of the prediciton window. In this case (if we have two input variables), the input dimensions to our model are $(N, w_c, 2)$. To include future data for the temperature variable, we can shift the temperature data by $k$ timepoints, s.t. the first measurement included in the context window would be from timepoint $t=k$, while the last timepoint would be at $t=w_c + k$. This way, the model can see $k$ steps into the future, at the cost of losing the first $k$ measurements for the temperature data.\n",
        "\n",
        "2. **Appending and Padding:** If we do not want to lose the information at the beginning of the temperature sequence, we can also append the whole future window for this variable to our context window. Then the data would be of dimensionality $(N, w_c + w_p, 2)$, where the last $w_p$ indices for ozone would be some padding value (probably best to pick an out-of-distribution value). If the model struggles with the large sequence of pads, the task could also be reformulated to a single-step prediciton task.\n",
        "\n",
        "(**A probably better apporach:** If we can make architectural changes, it would probably be a good idea to revert to an encoder-decoder architecture, where the _past_ data is encoded together, and future temperature data is only given to the decoder while predicting ozone values. In this way, we have an explicit distinction between past and future values, which can probably be learned more efficiently by the model.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46be3f8f",
      "metadata": {
        "id": "46be3f8f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Input\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "context_window = 336\n",
        "prediction_horizon = 96\n",
        "variable_column = [\"temp\", \"o3\"] # define the variables wanted for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c28ae27c",
      "metadata": {
        "id": "c28ae27c"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate model performance\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    return rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CguAzHMVBBdE",
      "metadata": {
        "id": "CguAzHMVBBdE"
      },
      "source": [
        "# Loading multi-variable data-sequence\n",
        "\n",
        "Here the same as in the original ozone-prediction notebook is done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dac2884",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dac2884",
        "outputId": "9d7c283e-7eb1-4932-e427-907b3e88e204"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_test_full shape: (160260, 336, 2), y_test_full shape: (160260, 96, 2)\n",
            "X_test shape: (160260, 336, 1), y_test shape: (160260, 96, 1)\n"
          ]
        }
      ],
      "source": [
        "from re import X\n",
        "import pickle\n",
        "\n",
        "# Load the prepared multi-variable data\n",
        "with open(\"X_train.pkl\", \"rb\") as f:\n",
        "    X_train_full = pickle.load(f)\n",
        "\n",
        "with open(\"X_test.pkl\", \"rb\") as f:\n",
        "    X_test_full = pickle.load(f)\n",
        "\n",
        "with open(\"y_train.pkl\", \"rb\") as f:\n",
        "    y_train_full = pickle.load(f)\n",
        "\n",
        "with open(\"y_test.pkl\", \"rb\") as f:\n",
        "    y_test_full = pickle.load(f)\n",
        "\n",
        "print(f\"X_train_full shape: {X_train_full.shape}, y_train_full shape: {y_train_full.shape}\")\n",
        "print(f\"X_test_full shape: {X_test_full.shape}, y_test_full shape: {y_test_full.shape}\")\n",
        "\n",
        "## Else if using local files:\n",
        "dataframe = pd.read_csv(\"normalized_data.csv\")\n",
        "scaler_stats = {col: {'mean': dataframe[col].mean(), 'std': dataframe[col].std()} for col in variable_column}\n",
        "\n",
        "\n",
        "# the station code is the first variable column, hence select only the last two\n",
        "X_train = X_train_full[:,:,1:].copy()\n",
        "X_test = X_test_full[:,:,1:].copy()\n",
        "\n",
        "# for the label, we only want the ozone data, which is the second column\n",
        "\n",
        "temp_y_train = y_train_full[:,:,1].copy()  # temperature data for training\n",
        "temp_y_test = y_test_full[:,:,1].copy()    # temperature data for testing\n",
        "\n",
        "y_train = y_train_full[:,:,2].copy()\n",
        "y_test = y_test_full[:,:,2].copy()\n",
        "\n",
        "X_train = np.array(X_train, dtype=np.float32)\n",
        "X_test = np.array(X_test, dtype=np.float32)\n",
        "y_train = np.array(y_train, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.float32)\n",
        "temp_y_train = np.array(temp_y_train, dtype=np.float32)\n",
        "temp_y_test = np.array(temp_y_test, dtype=np.float32)\n",
        "\n",
        "# verify the shapes of the data\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "125bf7d7",
      "metadata": {},
      "source": [
        "## Define Training Function\n",
        "\n",
        "The model is trained in the same way for both approaches, only the data preparation is different. The only thing we need to account for is the context-window length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92c3c60a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_lstm_model(X_train, y_train, context_window):\n",
        "\n",
        "    # Tunable LSTM parameters\n",
        "    lstm_units = 50\n",
        "    lstm_epochs = 5\n",
        "    lstm_batch_size = 16\n",
        "    lstm_optim = 'adam'\n",
        "    lstm_loss = 'mse'\n",
        "\n",
        "    checkpoint_dir = \"./checkpoint/\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f\"lstm_multivar.h5\")\n",
        "\n",
        "    ## Ignore user warning on keras as the choice for this exercise is to use h5.\n",
        "    print(f\"Training new model for variables {variable_column}\")\n",
        "\n",
        "    # the only change needed to allow for multiple input variables is to change the input shape of the LSTM layer\n",
        "    # to match the number of variables in the input data\n",
        "    lstm_model = Sequential([\n",
        "        LSTM(lstm_units, return_sequences=True, input_shape=(context_window, len(variable_column))), # change to allow mulitple input variables\n",
        "        LSTM(lstm_units, return_sequences=False),\n",
        "        Dense(prediction_horizon)\n",
        "    ])\n",
        "\n",
        "    lstm_model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        checkpoint_path, monitor=\"val_loss\", save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    training = lstm_model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=lstm_epochs, batch_size=lstm_batch_size,\n",
        "        validation_split=0.2, verbose=1,\n",
        "        callbacks=[checkpoint_callback]\n",
        "    )\n",
        "\n",
        "    training_history = training.history\n",
        "\n",
        "    return lstm_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c0b0ac2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_ozone_predictions(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Get ozone predictions from the trained model.\n",
        "    \"\"\"\n",
        "    lstm_pred = model.predict(X_test)\n",
        "    rmse = evaluate_model(y_test, y_test)\n",
        "    \n",
        "    return lstm_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6744cb5a",
      "metadata": {},
      "source": [
        "# Approach 1\n",
        "\n",
        "## Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "271b11b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "k = 24 # shift the temp data by 24 hours to use future temperature data\n",
        "\n",
        "temp_train = X_train[:, k:, 0].copy() # take the temperature data from the training set\n",
        "temp_future = temp_y_train[:, :k].copy() # take the temperature data from the future set\n",
        "\n",
        "concat_temp = np.concatenate((temp_train, temp_future), axis=1) # concatenate the two temperature data sets\n",
        "\n",
        "# replace temperature data in the training set with the concatenated data\n",
        "X_train1 = X_train.copy()  # create a copy of the training data to avoid modifying the original\n",
        "X_train1[:, :, 0] = concat_temp\n",
        "\n",
        "# train the LSTM model with the modified training data\n",
        "lstm_model = train_lstm_model(X_train1, y_train, context_window)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "865e3c37",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "61fd53e9",
      "metadata": {},
      "source": [
        "# Approach 2\n",
        "\n",
        "## Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c155ccfe",
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test_vars = y_test_full[:, :, 1:]\n",
        "\n",
        "X_train2 = np.concatenate((X_train1, y_test_vars), axis=1)  # concatenate the training data with the test data\n",
        "\n",
        "# mask out the ozone data in the training set\n",
        "X_train2[:, :, 1] = -99\n",
        "\n",
        "# Train the LSTM model with the modified training data\n",
        "lstm_model = train_lstm_model(X_train2, y_train, X_train2.shape[1])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
